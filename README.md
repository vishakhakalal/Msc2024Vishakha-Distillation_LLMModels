# Msc2024Vishakha-Distillation_LLMModels
Data contamination and Data leakage in LLM distilled models

This project investigates the impact of data contamination and data leakage on the performance of large language models (LLMs) and their distilled versions. By exploring various BERT-based architectures, the study aims to uncover how these issues affect model generalization, accuracy, and reliability.
